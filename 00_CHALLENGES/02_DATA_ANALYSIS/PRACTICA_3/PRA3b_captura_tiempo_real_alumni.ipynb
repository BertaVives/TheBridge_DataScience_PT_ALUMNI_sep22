{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ck4PufGcoNb2"
   },
   "source": [
    "# PEC 3 - Web Scraping Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KC-F1owwXIEv"
   },
   "source": [
    "En esta PEC vamos a **continuar trabajando el web scraping**. Vamos a prestar especial atenci√≥n al web scraping en streaming que es el objetivo del reto. Adem√°s, continuaremos explorando otras librer√≠as que nos permiten hacer web scraping, como request-html y SerPapi.\n",
    "\n",
    "Por tanto, la PEC se va a dividir en **3 PARTES**: Web Scraping en Streaming, Web Scraping con Requests-html y, Web Scraping con SerPapi.\n",
    "\n",
    "Mencionar que, en algunos ejercicios se va a motivar el uso de los selectores CSS y los XPath. \n",
    "**Los selectores CSS y XPath** son expresiones que permiten seleccionar elementos de un documento HTML basados en sus clases o en la ubicaci√≥n dentro del contenido. \n",
    "Una referencia interesante de los mismos la pod√©is encontrar en las siguientes dos p√°ginas web: https://www.w3schools.com/xml/xpath_syntax.asp, https://www.w3schools.com/cssref/css_selectors.asp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXE7YpuEXzMf"
   },
   "source": [
    "_Ejemplo:_\n",
    "\n",
    "\n",
    "\n",
    "*   _p.intro.rellevant_: seleccionar√≠a los elementos _p√°rrafo_ con valores de classe iguales a 'intro' y 'rellevant'.  \n",
    "*   _div > p_ : selecciona todos los elementos _\\<p>_ donde el padre sea un elemento \\<div>.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wRrPf7uoeG7"
   },
   "source": [
    "## Parte 2. Web Scraping con Requests-html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ps6A-eVUvLf9"
   },
   "source": [
    "La librer√≠a **request- Html**  es como una combinaci√≥n de la librer√≠a requests y BeautifulSoup. \n",
    "\n",
    "El punto m√°s fuerte es que tiene soporte completo para JavaScript, lo que significa que puede ejecutar JavaScript y nos permite, por tanto, hacer scraping de contenido generado din√°micamente.  Por ejemplo, una aplicaci√≥n muy com√∫n es acceder el contenido que est√° disponible en las p√°ginas siguientes a la primera, y que cuando navegamos accedemos a √©l presionando el bot√≥n de la p√°gina correspondiente.  \n",
    "\n",
    "Adem√°s, para ejecutar JavaScript, podemos usar tambi√©n el m√©todo render de la librer√≠a. \n",
    "\n",
    "Otra particularizaci√≥n de esta librer√≠a es que es necesario iniciar sesi√≥n antes de empezar con el scraping del contenido HTML y cerrar sesi√≥n cuando se termine. Es decir:\n",
    " \n",
    "\n",
    "\n",
    "```\n",
    "request_html importar HTMLSession\n",
    "session = HTMLSession\n",
    "r = session.get (url_base)\n",
    "r.html.render\n",
    "‚Ä¶.\n",
    "session.close()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaCWQacEsMfm"
   },
   "source": [
    "Adem√°s, esta librer√≠a permite seleccionar los elementos de los documentos html mediant selectores CSS y/o selectores XPath. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9AI42u9rmvA"
   },
   "source": [
    "La documentaci√≥n de esta librer√≠a, la cual es recomendable que revis√©is para trabajar esta parte, la pod√©is encontrar en el siguiente enlace:  \n",
    "\n",
    "https://requests.readthedocs.io/projects/requests-html/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MygGSU_quC3"
   },
   "source": [
    "Antes de empezar a trabajar con esta librer√≠a, es necesario instalarla puesto que Google Collab no la tiene instalada por defecto como ocurria con las librerias que hemos utilizado anteriormente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDtQuPn3qx0M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests-html\n",
      "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
      "Collecting parse\n",
      "  Downloading parse-1.19.0.tar.gz (30 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from requests-html) (2.28.1)\n",
      "Collecting w3lib\n",
      "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
      "Collecting pyquery\n",
      "  Downloading pyquery-1.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting fake-useragent\n",
      "  Downloading fake_useragent-1.1.1-py3-none-any.whl (50 kB)\n",
      "Collecting bs4\n",
      "  Using cached bs4-0.0.1-py3-none-any.whl\n",
      "Collecting pyppeteer>=0.0.14\n",
      "  Downloading pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: certifi>=2021 in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (2021.10.8)\n",
      "Collecting websockets<11.0,>=10.0\n",
      "  Downloading websockets-10.4-cp39-cp39-win_amd64.whl (101 kB)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (1.26.12)\n",
      "Collecting pyee<9.0.0,>=8.1.0\n",
      "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tqdm<5.0.0,>=4.42.1\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Collecting importlib-metadata>=1.4\n",
      "  Downloading importlib_metadata-5.1.0-py3-none-any.whl (21 kB)\n",
      "Collecting appdirs<2.0.0,>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.11.0-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from tqdm<5.0.0,>=4.42.1->pyppeteer>=0.0.14->requests-html) (0.4.4)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Using cached soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Collecting importlib-resources>=5.0\n",
      "  Downloading importlib_resources-5.10.1-py3-none-any.whl (34 kB)\n",
      "Collecting cssselect>0.7.9\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting lxml>=2.1\n",
      "  Downloading lxml-4.9.1-cp39-cp39-win_amd64.whl (3.6 MB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from requests->requests-html) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from requests->requests-html) (3.4)\n",
      "Building wheels for collected packages: parse\n",
      "  Building wheel for parse (setup.py): started\n",
      "  Building wheel for parse (setup.py): finished with status 'done'\n",
      "  Created wheel for parse: filename=parse-1.19.0-py3-none-any.whl size=24591 sha256=4cd677c4c98ee71ba68631e136f9cac894c29760bc0210916172146fecd14aeb\n",
      "  Stored in directory: c:\\users\\mrusso\\appdata\\local\\pip\\cache\\wheels\\d6\\9c\\58\\ee3ba36897e890f3ad81e9b730791a153fce20caa4a8a474df\n",
      "Successfully built parse\n",
      "Installing collected packages: zipp, soupsieve, websockets, tqdm, pyee, lxml, importlib-resources, importlib-metadata, cssselect, beautifulsoup4, appdirs, w3lib, pyquery, pyppeteer, parse, fake-useragent, bs4, requests-html\n",
      "Successfully installed appdirs-1.4.4 beautifulsoup4-4.11.1 bs4-0.0.1 cssselect-1.2.0 fake-useragent-1.1.1 importlib-metadata-5.1.0 importlib-resources-5.10.1 lxml-4.9.1 parse-1.19.0 pyee-8.2.2 pyppeteer-1.0.2 pyquery-1.4.3 requests-html-0.10.0 soupsieve-2.3.2.post1 tqdm-4.64.1 w3lib-2.1.1 websockets-10.4 zipp-3.11.0\n"
     ]
    }
   ],
   "source": [
    "# Instalar libreria \n",
    "!pip install requests-html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNm94Bb7tT8f"
   },
   "source": [
    "Una vez instalada, vamos a proceder con el primer ejemplo ilustrativo que nos servir√° como guia para realizar el ejercicio pr√°ctico planteado a continuaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cn_P5GQz2pTg"
   },
   "source": [
    "En este ejemplo, vamos a hacer scraping al contenido de la conocida p√°gina de noticias _Reddit_ (https://reddit.com). De ella, vamos a extraer los titulares y cu√°ntas votaciones tiene cada noticia. Adem√°s, vamos a hacer la selecci√≥n de este contenido mediante _selectores CSS_ para empezar a familiarizarnos con ellos.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4wbaQLN3b12"
   },
   "source": [
    "Si presetamos atenci√≥n a la p√°gina, podemos ver que hay contenido que se carga de forma din√°mica y/o mediante botones. Particularmente, el contenido se va cargando cuando hacemos \"scrolldown\", mientras que tambien podemos cargar el contenido asociado a una cuenta, mediante el boton \"Sign up\". Por tanto, utilizar la librer√≠a **requests_html** podr√≠a ser recomendable si se quisiera capturar el contenido html que se carga con alguna de estas opciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_b-AfHR6H6w"
   },
   "source": [
    "En primer lugar, vamos a cargar la librer√≠a e iniciar sesi√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzTXRo6Mthgx"
   },
   "outputs": [],
   "source": [
    "# Cargar librer√≠a\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "# Iniciar sesi√≥n\n",
    "session = HTMLSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGcC1WnP6h2M"
   },
   "source": [
    "Ahora, hacemos la solicitud y comprobamos cuantos html tenemos en la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "ubj5Yr1516Qv",
    "outputId": "a9b484c8-7dcc-43f8-9f4c-87e691278e1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HTML url='https://www.reddit.com/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32mc:\\Users\\mrusso\\OneDrive - ATSISTEMAS SA\\Documentos\\GitHub\\TheBridge_DataScience_PT_ALUMNI_sep22\\00_CHALLENGES\\02_DATA_ANALYSIS\\PRACTICA_3\\PRA3a_captura_tiempo_real_alumni.ipynb Celda 73\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mrusso/OneDrive%20-%20ATSISTEMAS%20SA/Documentos/GitHub/TheBridge_DataScience_PT_ALUMNI_sep22/00_CHALLENGES/02_DATA_ANALYSIS/PRACTICA_3/PRA3a_captura_tiempo_real_alumni.ipynb#Y132sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m r \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mhttps://reddit.com\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mrusso/OneDrive%20-%20ATSISTEMAS%20SA/Documentos/GitHub/TheBridge_DataScience_PT_ALUMNI_sep22/00_CHALLENGES/02_DATA_ANALYSIS/PRACTICA_3/PRA3a_captura_tiempo_real_alumni.ipynb#Y132sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m html \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39mhtml:\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mrusso/OneDrive%20-%20ATSISTEMAS%20SA/Documentos/GitHub/TheBridge_DataScience_PT_ALUMNI_sep22/00_CHALLENGES/02_DATA_ANALYSIS/PRACTICA_3/PRA3a_captura_tiempo_real_alumni.ipynb#Y132sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(html)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\requests_html.py:481\u001b[0m, in \u001b[0;36mHTML.__iter__\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    479\u001b[0m \u001b[39myield\u001b[39;00m \u001b[39mnext\u001b[39m\n",
      "\u001b[0;32m    480\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m--> 481\u001b[0m     \u001b[39mnext\u001b[39m \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m\u001b[39m.\u001b[39;49mnext(fetch\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, next_symbol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_symbol)\u001b[39m.\u001b[39mhtml\n",
      "\u001b[0;32m    482\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n",
      "\u001b[0;32m    483\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\requests_html.py:470\u001b[0m, in \u001b[0;36mHTML.next\u001b[1;34m(self, fetch, next_symbol)\u001b[0m\n",
      "\u001b[0;32m    467\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;32m    469\u001b[0m \u001b[39mif\u001b[39;00m fetch:\n",
      "\u001b[1;32m--> 470\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mget(url)\n",
      "\u001b[0;32m    471\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m    472\u001b[0m     \u001b[39mreturn\u001b[39;00m url\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\requests\\sessions.py:600\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    592\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n",
      "\u001b[0;32m    593\u001b[0m \n",
      "\u001b[0;32m    594\u001b[0m \u001b[39m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n",
      "\u001b[0;32m    595\u001b[0m \u001b[39m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n",
      "\u001b[0;32m    596\u001b[0m \u001b[39m:rtype: requests.Response\u001b[39;00m\n",
      "\u001b[0;32m    597\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    599\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32m--> 600\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest(\u001b[39m\"\u001b[39m\u001b[39mGET\u001b[39m\u001b[39m\"\u001b[39m, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n",
      "\u001b[0;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n",
      "\u001b[0;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n",
      "\u001b[0;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n",
      "\u001b[0;32m    585\u001b[0m }\n",
      "\u001b[0;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n",
      "\u001b[1;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n",
      "\u001b[0;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n",
      "\u001b[0;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n",
      "\u001b[1;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "\u001b[0;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\requests\\adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n",
      "\u001b[0;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n",
      "\u001b[1;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n",
      "\u001b[0;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n",
      "\u001b[0;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n",
      "\u001b[0;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n",
      "\u001b[0;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n",
      "\u001b[0;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n",
      "\u001b[0;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n",
      "\u001b[0;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n",
      "\u001b[0;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n",
      "\u001b[0;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n",
      "\u001b[0;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n",
      "\u001b[0;32m    500\u001b[0m         )\n",
      "\u001b[0;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n",
      "\u001b[0;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n",
      "\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n",
      "\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n",
      "\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n",
      "\u001b[0;32m    704\u001b[0m     conn,\n",
      "\u001b[0;32m    705\u001b[0m     method,\n",
      "\u001b[0;32m    706\u001b[0m     url,\n",
      "\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n",
      "\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n",
      "\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n",
      "\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n",
      "\u001b[0;32m    711\u001b[0m )\n",
      "\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n",
      "\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n",
      "\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n",
      "\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "\u001b[0;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\urllib3\\connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n",
      "\u001b[0;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n",
      "\u001b[0;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n",
      "\u001b[0;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n",
      "\u001b[0;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "\u001b[1;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n",
      "\u001b[0;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\urllib3\\connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n",
      "\u001b[0;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n",
      "\u001b[0;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n",
      "\u001b[0;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n",
      "\u001b[0;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n",
      "\u001b[0;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n",
      "\u001b[0;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "\u001b[0;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\http\\client.py:1345\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m   1343\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;32m   1344\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m-> 1345\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n",
      "\u001b[0;32m   1346\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
      "\u001b[0;32m   1347\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\http\\client.py:307\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    305\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n",
      "\u001b[0;32m    306\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "\u001b[1;32m--> 307\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n",
      "\u001b[0;32m    308\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
      "\u001b[0;32m    309\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\http\\client.py:268\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    267\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n",
      "\u001b[1;32m--> 268\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n",
      "\u001b[0;32m    270\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n",
      "\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n",
      "\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n",
      "\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n",
      "\u001b[0;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n",
      "\u001b[0;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n",
      "\u001b[0;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n",
      "\u001b[1;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n",
      "\u001b[0;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n",
      "\u001b[0;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n",
      "\u001b[0;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "r = session.get('https://reddit.com')\n",
    "for html in r.html:\n",
    "    print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_oi_bku2ItU"
   },
   "source": [
    "Podemos observar que el html que devuelve es solo 1. Esto es porque el contenido se va actualizando y a√±adiendo din√°micamente mediante \"scrolldown\" y no mediante el click de un boton de Pagina siguiente, Pagina 2, o similares. En caso de haber un segundo html, este hace referencia al boton de introducir una cuenta reddit. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kZWiN_p83uC"
   },
   "source": [
    "Ahora, despu√©s de inspeccionar la p√°gina, advertimos que el contenido relativo a los enlaces de las diferentes entradas o noticias de la p√°gina, est√° en la etiqueta 'a' y cuya classe es '_3ryJoIoycVkA88fy40qNJc'. Por tanto, usando selectores CSS, la instrucci√≥n que devolver√° el contenidos ser√°: *r.html.find('a._3ryJoIoycVkA88fy40qNJc')*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "YLYQts748d_-",
    "outputId": "38c57b0d-bccd-4a81-f051-29f65dbd4c47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 titulares aparecen en la primera p√°gina de reddit, el resto de titulares se corresponden con el contenido despu√©s de hacer scrolldowns\n"
     ]
    }
   ],
   "source": [
    "subreddit_1 = r.html.find('a._3ryJoIoycVkA88fy40qNJc')\n",
    "print(str(len(subreddit_1)) + ' titulares aparecen en la primera p√°gina de reddit, el resto de titulares se corresponden con el contenido despu√©s de hacer scrolldowns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "jO9Nthx2-gTK",
    "outputId": "292c4693-3d31-421d-eebd-61d25ff3f96f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'https://www.reddit.com/r/spain/'},\n",
       " {'https://www.reddit.com/r/Genshin_Impact_Leaks/'},\n",
       " {'https://www.reddit.com/r/AskReddit/'},\n",
       " {'https://www.reddit.com/r/LMDShow/'},\n",
       " {'https://www.reddit.com/r/interestingasfuck/'},\n",
       " {'https://www.reddit.com/r/Genshin_Impact_Leaks/'},\n",
       " {'https://www.reddit.com/r/mildlyinfuriating/'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtener las url completas de cada entrada\n",
    "subreddit_url=[element.absolute_links for element in subreddit_1]\n",
    "subreddit_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bsoi9zZS-xev"
   },
   "source": [
    "Ahora vamos a obtener los t√≠tulos de las noticias. Este contenido se halla en la etiqueta 'h3' con classe '_eYtD2XCVieq6emjKBH3m'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "ZGfKcOZ_-s4Y",
    "outputId": "999c2618-edf5-402a-8f41-246fee0f24ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ü§î',\n",
       " 'Um gajo pode sonhar n√£o pode?',\n",
       " 'Los Santos Drug Wars inyectar√° un nuevo caos psicoactivo en GTA Online el 13 de diciembre. √önete a una banda de inadaptados en el primer cap√≠tulo de una extensa nueva actualizaci√≥n de GTA Online, con una trama dividida en dos partes.',\n",
       " 'Alhaitham gameplay (R.I.P)',\n",
       " 'What did you not know about sex until you lost your virginity?',\n",
       " 'escucha escucha ‚òùÔ∏èü§ì',\n",
       " \"U.S. bombs dropped on Laos. 270 million bombs were dropped on Laos in a span of 9 years, making it the most heavily bombed country in the history of the world. That's 57 bombs every minute on average.\",\n",
       " 'Yaoyao Gameplay',\n",
       " 'The fees on this Air BnB for one night.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Titulos\n",
    "subreddit_2 = r.html.find('h3._eYtD2XCVieq6emjKBH3m')\n",
    "subreddit_titulos=[element.text for element in subreddit_2]\n",
    "subreddit_titulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "pnyQxO56_8qf",
    "outputId": "9bcd54c8-6847-47b4-a0e8-b0912c2e9ff4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['348', '‚Ä¢', '4.7k', '10.5k', '394', '18.1k', '2.0k', '14.3k']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Votaciones\n",
    "subreddit_3 = r.html.find('div._1rZYMD_4xY3gRcSS3p8ODO._25IkBM0rRUqWX5ZojEMAFQ')\n",
    "subreddit_votaciones=[element.text for element in subreddit_3]\n",
    "subreddit_votaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7VLcfGBHcHh"
   },
   "source": [
    "Por √∫ltimo, cerraremos la sesi√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EPYBbWQiHmAa"
   },
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7H66P1kAjHT"
   },
   "source": [
    "Ahora vamos a construir el dataframe que contiene el resultado de los t√≠tulos y las voraciones. Atendiendo a los datos, el primer titular sin puntuaci√≥n lo vamos a excluir porque hace referencia a un anuncio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QGsFYZbBboW"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (7) does not match length of index (8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "\u001b[1;32mc:\\Users\\mrusso\\OneDrive - ATSISTEMAS SA\\Documentos\\GitHub\\TheBridge_DataScience_PT_ALUMNI_sep22\\00_CHALLENGES\\02_DATA_ANALYSIS\\PRACTICA_3\\PRA3a_captura_tiempo_real_alumni.ipynb Celda 85\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mrusso/OneDrive%20-%20ATSISTEMAS%20SA/Documentos/GitHub/TheBridge_DataScience_PT_ALUMNI_sep22/00_CHALLENGES/02_DATA_ANALYSIS/PRACTICA_3/PRA3a_captura_tiempo_real_alumni.ipynb#Y146sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df_reddit\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mDataFrame()\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mrusso/OneDrive%20-%20ATSISTEMAS%20SA/Documentos/GitHub/TheBridge_DataScience_PT_ALUMNI_sep22/00_CHALLENGES/02_DATA_ANALYSIS/PRACTICA_3/PRA3a_captura_tiempo_real_alumni.ipynb#Y146sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df_reddit[\u001b[39m'\u001b[39m\u001b[39mt√≠tulo\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39msubreddit_titulos[\u001b[39m1\u001b[39m:]\n",
      "\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mrusso/OneDrive%20-%20ATSISTEMAS%20SA/Documentos/GitHub/TheBridge_DataScience_PT_ALUMNI_sep22/00_CHALLENGES/02_DATA_ANALYSIS/PRACTICA_3/PRA3a_captura_tiempo_real_alumni.ipynb#Y146sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m df_reddit[\u001b[39m'\u001b[39m\u001b[39mvotaciones\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39msubreddit_votaciones[\u001b[39m1\u001b[39m:]\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mrusso/OneDrive%20-%20ATSISTEMAS%20SA/Documentos/GitHub/TheBridge_DataScience_PT_ALUMNI_sep22/00_CHALLENGES/02_DATA_ANALYSIS/PRACTICA_3/PRA3a_captura_tiempo_real_alumni.ipynb#Y146sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m df_reddit[\u001b[39m'\u001b[39m\u001b[39murl\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39msubreddit_url\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\pandas\\core\\frame.py:3978\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n",
      "\u001b[0;32m   3975\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n",
      "\u001b[0;32m   3976\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m   3977\u001b[0m     \u001b[39m# set column\u001b[39;00m\n",
      "\u001b[1;32m-> 3978\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\pandas\\core\\frame.py:4172\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n",
      "\u001b[0;32m   4162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;32m   4163\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m   4164\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n",
      "\u001b[0;32m   4165\u001b[0m \n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m   4170\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n",
      "\u001b[0;32m   4171\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;32m-> 4172\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n",
      "\u001b[0;32m   4174\u001b[0m     \u001b[39mif\u001b[39;00m (\n",
      "\u001b[0;32m   4175\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n",
      "\u001b[0;32m   4176\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;32m   4177\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n",
      "\u001b[0;32m   4178\u001b[0m     ):\n",
      "\u001b[0;32m   4179\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n",
      "\u001b[0;32m   4180\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\pandas\\core\\frame.py:4905\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n",
      "\u001b[0;32m   4902\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n",
      "\u001b[0;32m   4904\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n",
      "\u001b[1;32m-> 4905\u001b[0m     com\u001b[39m.\u001b[39;49mrequire_length_match(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n",
      "\u001b[0;32m   4906\u001b[0m \u001b[39mreturn\u001b[39;00m sanitize_array(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_2d\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\pandas\\core\\common.py:561\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n",
      "\u001b[0;32m    557\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    558\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n",
      "\u001b[0;32m    559\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    560\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n",
      "\u001b[1;32m--> 561\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n",
      "\u001b[0;32m    562\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m    563\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m    564\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m    565\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m    566\u001b[0m     )\n",
      "\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (7) does not match length of index (8)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_reddit=pd.DataFrame()\n",
    "df_reddit['t√≠tulo']=subreddit_titulos[1:]\n",
    "df_reddit['votaciones']=subreddit_votaciones[1:]\n",
    "df_reddit['url']=subreddit_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "DaWKTgJiB02L",
    "outputId": "9cd16d1f-f791-4c79-f022-553cf6f30333"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t√≠tulo</th>\n",
       "      <th>votaciones</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masks that fit your face and your personality. Shop Now</td>\n",
       "      <td>1.1k</td>\n",
       "      <td>{https://www.reddit.com/r/leagueoflegends/}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is a masterpiece</td>\n",
       "      <td>28.6k</td>\n",
       "      <td>{https://www.reddit.com/r/Damnthatsinteresting/}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Truck drivers, what's a creepy story you've got from the middle of nowhere?</td>\n",
       "      <td>45.9k</td>\n",
       "      <td>{https://www.reddit.com/r/AskReddit/}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE HOUSTON ASTROS HAVE BEEN ELIMINATED FROM 2020 WORLD SERIES CONTENTION</td>\n",
       "      <td>42.9k</td>\n",
       "      <td>{https://www.reddit.com/r/baseball/}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dad of the year</td>\n",
       "      <td>20.5k</td>\n",
       "      <td>{https://www.reddit.com/r/nextfuckinglevel/}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Happy 18th birthday. Buffy can officially legally drink in Australia</td>\n",
       "      <td>37.9k</td>\n",
       "      <td>{https://www.reddit.com/r/aww/}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        t√≠tulo  ...                                               url\n",
       "0  Masks that fit your face and your personality. Shop Now                      ...  {https://www.reddit.com/r/leagueoflegends/}     \n",
       "1  this is a masterpiece                                                        ...  {https://www.reddit.com/r/Damnthatsinteresting/}\n",
       "2  Truck drivers, what's a creepy story you've got from the middle of nowhere?  ...  {https://www.reddit.com/r/AskReddit/}           \n",
       "3  THE HOUSTON ASTROS HAVE BEEN ELIMINATED FROM 2020 WORLD SERIES CONTENTION    ...  {https://www.reddit.com/r/baseball/}            \n",
       "4  Dad of the year                                                              ...  {https://www.reddit.com/r/nextfuckinglevel/}    \n",
       "5  Happy 18th birthday. Buffy can officially legally drink in Australia         ...  {https://www.reddit.com/r/aww/}                 \n",
       "\n",
       "[6 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df_reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tD5CuNtYxtPb"
   },
   "source": [
    "En este ejemplo, hemos podido scrapear el contenido inicial que aparece en la p√°gina de Reddit. No obstante, el contenido que encontramos al hacer _scrolldown_ no lo hemos podido capturar.  \n",
    "\n",
    "Para poder capturar el resto de entradas, podemos hacer uso de la funcion *render()* de la librer√≠a *Request-html*. No obstante, para el uso de esta funci√≥n es necesario iniciar una sesi√≥n en modo asincrona. Los Jupyter notebooks presentan ciertos problemas para trabajar de esta forma y requieren la instalaci√≥n de _Chromium_; es por eso que en esta PEC no lo vamos a ver. A pesar de ello, a modo explicativo, se adjunta el c√≥digo que se requerir√≠a:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "import asyncio\n",
    "from requests_html import AsyncHTMLSession\n",
    "\n",
    "asession = AsyncHTMLSession()\n",
    "\n",
    "async def get_results():\n",
    "    r = await asession.get('https://reddit.com')\n",
    "    r.html.arender(scrolldown=10, sleep=1)\n",
    "    return r\n",
    "\n",
    "respuesta = asession.run(get_results)\n",
    "\n",
    "asession.close()\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p95Elq6CIWDQ"
   },
   "source": [
    "Otro ejemplo es la obtenci√≥n de informaci√≥n de una p√°gina que recoge los memes m√°s relevantes en la actualidad (www.knowyourmeme.com). En esta p√°gina, a diferencia del ejemplo anterior, el contenido est√° organizado por p√°ginas y se puede acceder a ellas a trav√©s de los t√≠picos botones de p√°agina 1, 2, 3,... Vamos a ver, por tanto, la respuesta que nos devolver√≠a requests-html en este caso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KRonTtQKoHk"
   },
   "outputs": [],
   "source": [
    "# Cargar librer√≠a\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "# Iniciar sesi√≥n\n",
    "session = HTMLSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "UwVnuDSNLOX1",
    "outputId": "a16a1e0d-796f-4d8b-a3dd-48558e4c85aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :  <HTML url='https://knowyourmeme.com/'>\n",
      "2 :  <HTML url='https://knowyourmeme.com/page/2'>\n",
      "3 :  <HTML url='https://knowyourmeme.com/page/3'>\n",
      "4 :  <HTML url='https://knowyourmeme.com/page/4'>\n",
      "5 :  <HTML url='https://knowyourmeme.com/page/5'>\n",
      "6 :  <HTML url='https://knowyourmeme.com/page/6'>\n",
      "7 :  <HTML url='https://knowyourmeme.com/page/7'>\n",
      "8 :  <HTML url='https://knowyourmeme.com/page/8'>\n",
      "9 :  <HTML url='https://knowyourmeme.com/page/9'>\n",
      "10 :  <HTML url='https://knowyourmeme.com/page/10'>\n"
     ]
    }
   ],
   "source": [
    "r2= session.get('https://knowyourmeme.com/')\n",
    "\n",
    "total_pags_scrap=10\n",
    "page=0\n",
    "for html in r2.html:\n",
    "    page+=1\n",
    "    print(page, ': ' ,html)\n",
    "    if page==total_pags_scrap:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxHydcKDMkKn"
   },
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1LJBx70NqQ4"
   },
   "source": [
    "En este caso, la respuesta contine los html asociados a las diferentes p√°ginas en las que est√° organizado el contenido. Como pod√©is observar, en el bucle se ha introducido un _break_. Esto es porque knowyourmeme.com ofrece la posibilidad de revisar los memes contenidos en  hasta m√°s de 9500 p√°ginas. Esperar a scrapear este total de p√°ginas nos llevar√≠a mucho m√°s tiempo y no es el objetivo de esta PEC.\n",
    "\n",
    "Considerando los ejemplos que se han facilitado, realizar el ejercicio pr√°ctico 3 para explorar la librer√≠a Requests-html y familiarizarse con los selectores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9r0z0MdxbSs4"
   },
   "source": [
    "### **Ejercicio pr√°ctico 2 (vuelos real time)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTucSQJybd9F"
   },
   "source": [
    "La API OpenSky (https://opensky-network.org/apidoc/rest.html) permite recuperar informaci√≥n del estado de los vuelos o aeronaves actualizados en tiempo real. Adem√°s, los usuarios con autorizaci√≥n pueden tener acceso a datos hist√≥ricos de la √∫ltima hora, indicando el intervalo de tiempo c√≥mo un par√°metro m√°s en la consulta. Para esta actividad vamos a utilizar la versi√≥n b√°sica de esta API y por tanto, no vamos a tener opci√≥n de seleccionar el intervalo de tiempo de inter√©s. No obstante, para la finalidad de la actividad, no va a hacer falta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwHLlsi7bjiW"
   },
   "source": [
    "En este ejercicio se solicita al estudiante que, despu√©s de la revisi√≥n de la documentaci√≥n de la API, obtenga el estado del aire (aviones y aeronaves) durante una secuencia de 10 consultas. Para ello, se recomienda que se implente un bucle donde en cada iteraci√≥n se solicite informaci√≥n a la API y se esperen 10 segundos entre una iteraci√≥n y la siguiente. La solicitud a la API se pide que se realice mediante la librer√≠a requests, con la que se trabaj√≥ en la PEC anterior. \n",
    "\n",
    "Mencionar que esta API permite a√±adir en la solicitud una acotaci√≥n del espacio a considerar. La acotaci√≥n del espacio a√©reo se corresponder√° con los siguientes valores de latitud i longitud:\n",
    "\n",
    "> lat_max= 72.822950; \n",
    "lon_min= -17.000158;\n",
    "lat_min= 35.550423;\n",
    "lon_max= 44.022436\n",
    "\n",
    "Para cada consulta, se solicita obtener el n√∫mero total de aeronaves agrupadas por pa√≠s de origen de la aeronave. Para ello, ser√° necesario revisar la documentaci√≥n de la API y explorar la forma de la respuesta para determinar cu√°l es el campo de inter√©s de los datos scrapeados para la realizaci√≥n del ejercicio. \n",
    "Adem√°s del n√∫mero de aeronaves por pa√≠s de origen, en cada iteraci√≥n tambi√©n hay que almacenar el valor temporal de la consulta (es decir, el campo 'time', que viene dado en segundos ).\n",
    "\n",
    "Con el resultado de las 10 iteraciones, se solicita que:\n",
    "- Se cree un dataframe (df_vuelos) que recoja el n√∫mero total de aeronaves por cada pa√≠s de origen (filas) para cada instante de tiempo (columnas). \n",
    "- Se a√±ada una columna 'mean_flights' y una 'percen_flights' que representen respectivamente el valor medio de vuelos de cada pa√≠s durante el intervalo de tiempo considerado con las 10 iteraciones y el porcentaje del pa√≠s respecto al total.\n",
    "- Representar con un diagrama de barras el n√∫mero de vuelos medios de los 15 pa√≠ses con mayor media de vuelos en el intervalo de tiempo considerado. El eje X, se corresponder√° con el pa√≠s y el eje Y con el valor de vuelos medios. Mostrar los valores ordenados de forma ascendiente. Mostrar tambi√©n el porcentaje agregado de estos 15 pa√≠ses respecto al total.\n",
    "- Repetir el putno anterior para los 15 pa√≠ses con menor media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deZfeuJ6n67S"
   },
   "source": [
    "NOTAS PARA LA REALIZACI√ìN DE LAS SOLICITUDES Y LA CREACI√ìN DEL DATAFRAME: \n",
    "- Para considerar el tiempo de espera, como ya hemos hecho en casos anteriores, se recomienda el uso de la librer√≠a time y su funci√≥n time.sleep(n_segundos).\n",
    "- Debido a que estamos usando la versi√≥n gratuita de la API, el tiempo no se actualiza de forma constante y puede que haya solicitudes cuyo valor de 'time' es el mismo. En este caso, no almacenar el valor o eliminar despu√©s los valores cuyo 'time' est√© duplicado. Esto es para que cuando calculemos la media, no tenga efecto en la misma el valor duplicado.\n",
    "- _Sugerencia:_ En cada iteraci√≥n se puede crear un dataframe auxiliar con los resultados de la solicitud y cuyas columnas sean 'Country' y 'N' para el 'time' evaluado en dicha iteraci√≥n. Despu√©s, con este dataframe auxiliar, ir hacido join o merge al dataframe glogal que contendr√° los resultados de todas las iteraciones. Si se sigue esta sugerencia, utilizar la versi√≥n 'outer' de la funci√≥n merge de fomra que se consideren todos los pa√≠ses que han aparecido en todas las iteraciones aunque en una determinada iteraci√≥n no hubiese aeronaves de alguno de ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEEfCaAvb69-"
   },
   "outputs": [],
   "source": [
    "#Cargar librerias\n",
    "import requests\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir limites Europa\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hs1Hq70Tb_DU"
   },
   "outputs": [],
   "source": [
    "# Creaci√≥n de df_vuelos mediante la realizaci√≥n de 10 solicitudes\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostrar df_vuelos\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear nuevas variables: 'mean_flights' y 'percen_flights'\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafico de barras (15 pa√≠ses con m√°s vuelos) + calcular porcentaje de esos 15 primeros paises\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZgLoHcjcBWj"
   },
   "outputs": [],
   "source": [
    "# Grafico de barras (15 pa√≠ses con menos vuelos)\n",
    "#TODO\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ZYIvtVsuFTrD"
   ],
   "name": "CyPD_PEC3_2020_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('the_bridge_22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c79a6bee0cc3880bade9bffe4f2ea0bcfc562fd81915ef930f986f3657b2ce7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
