{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ck4PufGcoNb2"
   },
   "source": [
    "# PEC 3 - Web Scraping Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KC-F1owwXIEv"
   },
   "source": [
    "En esta PEC vamos a **continuar trabajando el web scraping**. Vamos a prestar especial atención al web scraping en streaming que es el objetivo del reto. Además, continuaremos explorando otras librerías que nos permiten hacer web scraping, como request-html y SerPapi.\n",
    "\n",
    "Por tanto, la PEC se va a dividir en **3 PARTES**: Web Scraping en Streaming, Web Scraping con Requests-html y, Web Scraping con SerPapi.\n",
    "\n",
    "Mencionar que, en algunos ejercicios se va a motivar el uso de los selectores CSS y los XPath. \n",
    "**Los selectores CSS y XPath** son expresiones que permiten seleccionar elementos de un documento HTML basados en sus clases o en la ubicación dentro del contenido. \n",
    "Una referencia interesante de los mismos la podéis encontrar en las siguientes dos páginas web: https://www.w3schools.com/xml/xpath_syntax.asp, https://www.w3schools.com/cssref/css_selectors.asp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXE7YpuEXzMf"
   },
   "source": [
    "_Ejemplo:_\n",
    "\n",
    "\n",
    "\n",
    "*   _p.intro.rellevant_: seleccionaría los elementos _párrafo_ con valores de classe iguales a 'intro' y 'rellevant'.  \n",
    "*   _div > p_ : selecciona todos los elementos _\\<p>_ donde el padre sea un elemento \\<div>.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wRrPf7uoeG7"
   },
   "source": [
    "## Parte 2. Web Scraping con Requests-html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ps6A-eVUvLf9"
   },
   "source": [
    "La librería **request- Html**  es como una combinación de la librería requests y BeautifulSoup. \n",
    "\n",
    "El punto más fuerte es que tiene soporte completo para JavaScript, lo que significa que puede ejecutar JavaScript y nos permite, por tanto, hacer scraping de contenido generado dinámicamente.  Por ejemplo, una aplicación muy común es acceder el contenido que está disponible en las páginas siguientes a la primera, y que cuando navegamos accedemos a él presionando el botón de la página correspondiente.  \n",
    "\n",
    "Además, para ejecutar JavaScript, podemos usar también el método render de la librería. \n",
    "\n",
    "Otra particularización de esta librería es que es necesario iniciar sesión antes de empezar con el scraping del contenido HTML y cerrar sesión cuando se termine. Es decir:\n",
    " \n",
    "\n",
    "\n",
    "```\n",
    "request_html importar HTMLSession\n",
    "session = HTMLSession\n",
    "r = session.get (url_base)\n",
    "r.html.render\n",
    "….\n",
    "session.close()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaCWQacEsMfm"
   },
   "source": [
    "Además, esta librería permite seleccionar los elementos de los documentos html mediant selectores CSS y/o selectores XPath. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9AI42u9rmvA"
   },
   "source": [
    "La documentación de esta librería, la cual es recomendable que reviséis para trabajar esta parte, la podéis encontrar en el siguiente enlace:  \n",
    "\n",
    "https://requests.readthedocs.io/projects/requests-html/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MygGSU_quC3"
   },
   "source": [
    "Antes de empezar a trabajar con esta librería, es necesario instalarla puesto que Google Collab no la tiene instalada por defecto como ocurria con las librerias que hemos utilizado anteriormente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDtQuPn3qx0M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests-html\n",
      "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
      "Collecting parse\n",
      "  Downloading parse-1.19.0.tar.gz (30 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from requests-html) (2.28.1)\n",
      "Collecting w3lib\n",
      "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
      "Collecting pyquery\n",
      "  Downloading pyquery-1.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting fake-useragent\n",
      "  Downloading fake_useragent-1.1.1-py3-none-any.whl (50 kB)\n",
      "Collecting bs4\n",
      "  Using cached bs4-0.0.1-py3-none-any.whl\n",
      "Collecting pyppeteer>=0.0.14\n",
      "  Downloading pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: certifi>=2021 in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (2021.10.8)\n",
      "Collecting websockets<11.0,>=10.0\n",
      "  Downloading websockets-10.4-cp39-cp39-win_amd64.whl (101 kB)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (1.26.12)\n",
      "Collecting pyee<9.0.0,>=8.1.0\n",
      "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tqdm<5.0.0,>=4.42.1\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Collecting importlib-metadata>=1.4\n",
      "  Downloading importlib_metadata-5.1.0-py3-none-any.whl (21 kB)\n",
      "Collecting appdirs<2.0.0,>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.11.0-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from tqdm<5.0.0,>=4.42.1->pyppeteer>=0.0.14->requests-html) (0.4.4)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Using cached soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Collecting importlib-resources>=5.0\n",
      "  Downloading importlib_resources-5.10.1-py3-none-any.whl (34 kB)\n",
      "Collecting cssselect>0.7.9\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting lxml>=2.1\n",
      "  Downloading lxml-4.9.1-cp39-cp39-win_amd64.whl (3.6 MB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from requests->requests-html) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from requests->requests-html) (3.4)\n",
      "Building wheels for collected packages: parse\n",
      "  Building wheel for parse (setup.py): started\n",
      "  Building wheel for parse (setup.py): finished with status 'done'\n",
      "  Created wheel for parse: filename=parse-1.19.0-py3-none-any.whl size=24591 sha256=4cd677c4c98ee71ba68631e136f9cac894c29760bc0210916172146fecd14aeb\n",
      "  Stored in directory: c:\\users\\mrusso\\appdata\\local\\pip\\cache\\wheels\\d6\\9c\\58\\ee3ba36897e890f3ad81e9b730791a153fce20caa4a8a474df\n",
      "Successfully built parse\n",
      "Installing collected packages: zipp, soupsieve, websockets, tqdm, pyee, lxml, importlib-resources, importlib-metadata, cssselect, beautifulsoup4, appdirs, w3lib, pyquery, pyppeteer, parse, fake-useragent, bs4, requests-html\n",
      "Successfully installed appdirs-1.4.4 beautifulsoup4-4.11.1 bs4-0.0.1 cssselect-1.2.0 fake-useragent-1.1.1 importlib-metadata-5.1.0 importlib-resources-5.10.1 lxml-4.9.1 parse-1.19.0 pyee-8.2.2 pyppeteer-1.0.2 pyquery-1.4.3 requests-html-0.10.0 soupsieve-2.3.2.post1 tqdm-4.64.1 w3lib-2.1.1 websockets-10.4 zipp-3.11.0\n"
     ]
    }
   ],
   "source": [
    "# Instalar libreria \n",
    "!pip install requests-html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNm94Bb7tT8f"
   },
   "source": [
    "Una vez instalada, vamos a proceder con el primer ejemplo ilustrativo que nos servirá como guia para realizar el ejercicio práctico planteado a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cn_P5GQz2pTg"
   },
   "source": [
    "En este ejemplo, vamos a hacer scraping al contenido de la conocida página de noticias _Reddit_ (https://reddit.com). De ella, vamos a extraer los titulares y cuántas votaciones tiene cada noticia. Además, vamos a hacer la selección de este contenido mediante _selectores CSS_ para empezar a familiarizarnos con ellos.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4wbaQLN3b12"
   },
   "source": [
    "Si presetamos atención a la página, podemos ver que hay contenido que se carga de forma dinámica y/o mediante botones. Particularmente, el contenido se va cargando cuando hacemos \"scrolldown\", mientras que tambien podemos cargar el contenido asociado a una cuenta, mediante el boton \"Sign up\". Por tanto, utilizar la librería **requests_html** podría ser recomendable si se quisiera capturar el contenido html que se carga con alguna de estas opciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_b-AfHR6H6w"
   },
   "source": [
    "En primer lugar, vamos a cargar la librería e iniciar sesión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzTXRo6Mthgx"
   },
   "outputs": [],
   "source": [
    "# Cargar librería\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "# Iniciar sesión\n",
    "session = HTMLSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGcC1WnP6h2M"
   },
   "source": [
    "Ahora, hacemos la solicitud y comprobamos cuantos html tenemos en la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "ubj5Yr1516Qv",
    "outputId": "a9b484c8-7dcc-43f8-9f4c-87e691278e1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HTML url='https://www.reddit.com/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32mc:\\Users\\mrusso\\OneDrive - ATSISTEMAS SA\\Documentos\\GitHub\\TheBridge_DataScience_PT_ALUMNI_sep22\\00_CHALLENGES\\02_DATA_ANALYSIS\\PRACTICA_3\\PRA3a_captura_tiempo_real_alumni.ipynb Celda 73\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mrusso/OneDrive%20-%20ATSISTEMAS%20SA/Documentos/GitHub/TheBridge_DataScience_PT_ALUMNI_sep22/00_CHALLENGES/02_DATA_ANALYSIS/PRACTICA_3/PRA3a_captura_tiempo_real_alumni.ipynb#Y132sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m r \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mhttps://reddit.com\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mrusso/OneDrive%20-%20ATSISTEMAS%20SA/Documentos/GitHub/TheBridge_DataScience_PT_ALUMNI_sep22/00_CHALLENGES/02_DATA_ANALYSIS/PRACTICA_3/PRA3a_captura_tiempo_real_alumni.ipynb#Y132sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m html \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39mhtml:\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mrusso/OneDrive%20-%20ATSISTEMAS%20SA/Documentos/GitHub/TheBridge_DataScience_PT_ALUMNI_sep22/00_CHALLENGES/02_DATA_ANALYSIS/PRACTICA_3/PRA3a_captura_tiempo_real_alumni.ipynb#Y132sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(html)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\requests_html.py:481\u001b[0m, in \u001b[0;36mHTML.__iter__\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    479\u001b[0m \u001b[39myield\u001b[39;00m \u001b[39mnext\u001b[39m\n",
      "\u001b[0;32m    480\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m--> 481\u001b[0m     \u001b[39mnext\u001b[39m \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m\u001b[39m.\u001b[39;49mnext(fetch\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, next_symbol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_symbol)\u001b[39m.\u001b[39mhtml\n",
      "\u001b[0;32m    482\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n",
      "\u001b[0;32m    483\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\requests_html.py:470\u001b[0m, in \u001b[0;36mHTML.next\u001b[1;34m(self, fetch, next_symbol)\u001b[0m\n",
      "\u001b[0;32m    467\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;32m    469\u001b[0m \u001b[39mif\u001b[39;00m fetch:\n",
      "\u001b[1;32m--> 470\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mget(url)\n",
      "\u001b[0;32m    471\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m    472\u001b[0m     \u001b[39mreturn\u001b[39;00m url\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\requests\\sessions.py:600\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    592\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n",
      "\u001b[0;32m    593\u001b[0m \n",
      "\u001b[0;32m    594\u001b[0m \u001b[39m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n",
      "\u001b[0;32m    595\u001b[0m \u001b[39m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n",
      "\u001b[0;32m    596\u001b[0m \u001b[39m:rtype: requests.Response\u001b[39;00m\n",
      "\u001b[0;32m    597\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    599\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32m--> 600\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest(\u001b[39m\"\u001b[39m\u001b[39mGET\u001b[39m\u001b[39m\"\u001b[39m, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n",
      "\u001b[0;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n",
      "\u001b[0;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n",
      "\u001b[0;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n",
      "\u001b[0;32m    585\u001b[0m }\n",
      "\u001b[0;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n",
      "\u001b[1;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n",
      "\u001b[0;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n",
      "\u001b[0;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n",
      "\u001b[1;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "\u001b[0;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\requests\\adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n",
      "\u001b[0;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n",
      "\u001b[1;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n",
      "\u001b[0;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n",
      "\u001b[0;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n",
      "\u001b[0;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n",
      "\u001b[0;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n",
      "\u001b[0;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n",
      "\u001b[0;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n",
      "\u001b[0;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n",
      "\u001b[0;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n",
      "\u001b[0;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n",
      "\u001b[0;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n",
      "\u001b[0;32m    500\u001b[0m         )\n",
      "\u001b[0;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n",
      "\u001b[0;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n",
      "\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n",
      "\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n",
      "\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n",
      "\u001b[0;32m    704\u001b[0m     conn,\n",
      "\u001b[0;32m    705\u001b[0m     method,\n",
      "\u001b[0;32m    706\u001b[0m     url,\n",
      "\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n",
      "\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n",
      "\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n",
      "\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n",
      "\u001b[0;32m    711\u001b[0m )\n",
      "\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n",
      "\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n",
      "\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n",
      "\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "\u001b[0;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\urllib3\\connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n",
      "\u001b[0;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n",
      "\u001b[0;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n",
      "\u001b[0;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n",
      "\u001b[0;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "\u001b[1;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n",
      "\u001b[0;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\urllib3\\connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n",
      "\u001b[0;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n",
      "\u001b[0;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n",
      "\u001b[0;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n",
      "\u001b[0;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n",
      "\u001b[0;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n",
      "\u001b[0;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "\u001b[0;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\http\\client.py:1345\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m   1343\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;32m   1344\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m-> 1345\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n",
      "\u001b[0;32m   1346\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
      "\u001b[0;32m   1347\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\http\\client.py:307\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    305\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n",
      "\u001b[0;32m    306\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "\u001b[1;32m--> 307\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n",
      "\u001b[0;32m    308\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
      "\u001b[0;32m    309\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\http\\client.py:268\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    267\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n",
      "\u001b[1;32m--> 268\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n",
      "\u001b[0;32m    270\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n",
      "\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n",
      "\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n",
      "\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n",
      "\u001b[0;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n",
      "\u001b[0;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n",
      "\u001b[0;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n",
      "\u001b[1;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n",
      "\u001b[0;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n",
      "\u001b[0;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n",
      "\u001b[0;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "r = session.get('https://reddit.com')\n",
    "for html in r.html:\n",
    "    print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_oi_bku2ItU"
   },
   "source": [
    "Podemos observar que el html que devuelve es solo 1. Esto es porque el contenido se va actualizando y añadiendo dinámicamente mediante \"scrolldown\" y no mediante el click de un boton de Pagina siguiente, Pagina 2, o similares. En caso de haber un segundo html, este hace referencia al boton de introducir una cuenta reddit. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kZWiN_p83uC"
   },
   "source": [
    "Ahora, después de inspeccionar la página, advertimos que el contenido relativo a los enlaces de las diferentes entradas o noticias de la página, está en la etiqueta 'a' y cuya classe es '_3ryJoIoycVkA88fy40qNJc'. Por tanto, usando selectores CSS, la instrucción que devolverá el contenidos será: *r.html.find('a._3ryJoIoycVkA88fy40qNJc')*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "YLYQts748d_-",
    "outputId": "38c57b0d-bccd-4a81-f051-29f65dbd4c47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 titulares aparecen en la primera página de reddit, el resto de titulares se corresponden con el contenido después de hacer scrolldowns\n"
     ]
    }
   ],
   "source": [
    "subreddit_1 = r.html.find('a._3ryJoIoycVkA88fy40qNJc')\n",
    "print(str(len(subreddit_1)) + ' titulares aparecen en la primera página de reddit, el resto de titulares se corresponden con el contenido después de hacer scrolldowns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "jO9Nthx2-gTK",
    "outputId": "292c4693-3d31-421d-eebd-61d25ff3f96f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'https://www.reddit.com/r/spain/'},\n",
       " {'https://www.reddit.com/r/Genshin_Impact_Leaks/'},\n",
       " {'https://www.reddit.com/r/AskReddit/'},\n",
       " {'https://www.reddit.com/r/LMDShow/'},\n",
       " {'https://www.reddit.com/r/interestingasfuck/'},\n",
       " {'https://www.reddit.com/r/Genshin_Impact_Leaks/'},\n",
       " {'https://www.reddit.com/r/mildlyinfuriating/'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtener las url completas de cada entrada\n",
    "subreddit_url=[element.absolute_links for element in subreddit_1]\n",
    "subreddit_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bsoi9zZS-xev"
   },
   "source": [
    "Ahora vamos a obtener los títulos de las noticias. Este contenido se halla en la etiqueta 'h3' con classe '_eYtD2XCVieq6emjKBH3m'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "ZGfKcOZ_-s4Y",
    "outputId": "999c2618-edf5-402a-8f41-246fee0f24ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['🤔',\n",
       " 'Um gajo pode sonhar não pode?',\n",
       " 'Los Santos Drug Wars inyectará un nuevo caos psicoactivo en GTA Online el 13 de diciembre. Únete a una banda de inadaptados en el primer capítulo de una extensa nueva actualización de GTA Online, con una trama dividida en dos partes.',\n",
       " 'Alhaitham gameplay (R.I.P)',\n",
       " 'What did you not know about sex until you lost your virginity?',\n",
       " 'escucha escucha ☝️🤓',\n",
       " \"U.S. bombs dropped on Laos. 270 million bombs were dropped on Laos in a span of 9 years, making it the most heavily bombed country in the history of the world. That's 57 bombs every minute on average.\",\n",
       " 'Yaoyao Gameplay',\n",
       " 'The fees on this Air BnB for one night.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Titulos\n",
    "subreddit_2 = r.html.find('h3._eYtD2XCVieq6emjKBH3m')\n",
    "subreddit_titulos=[element.text for element in subreddit_2]\n",
    "subreddit_titulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "pnyQxO56_8qf",
    "outputId": "9bcd54c8-6847-47b4-a0e8-b0912c2e9ff4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['348', '•', '4.7k', '10.5k', '394', '18.1k', '2.0k', '14.3k']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Votaciones\n",
    "subreddit_3 = r.html.find('div._1rZYMD_4xY3gRcSS3p8ODO._25IkBM0rRUqWX5ZojEMAFQ')\n",
    "subreddit_votaciones=[element.text for element in subreddit_3]\n",
    "subreddit_votaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7VLcfGBHcHh"
   },
   "source": [
    "Por último, cerraremos la sesión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EPYBbWQiHmAa"
   },
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7H66P1kAjHT"
   },
   "source": [
    "Ahora vamos a construir el dataframe que contiene el resultado de los títulos y las voraciones. Atendiendo a los datos, el primer titular sin puntuación lo vamos a excluir porque hace referencia a un anuncio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QGsFYZbBboW"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (7) does not match length of index (8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "\u001b[1;32mc:\\Users\\mrusso\\OneDrive - ATSISTEMAS SA\\Documentos\\GitHub\\TheBridge_DataScience_PT_ALUMNI_sep22\\00_CHALLENGES\\02_DATA_ANALYSIS\\PRACTICA_3\\PRA3a_captura_tiempo_real_alumni.ipynb Celda 85\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mrusso/OneDrive%20-%20ATSISTEMAS%20SA/Documentos/GitHub/TheBridge_DataScience_PT_ALUMNI_sep22/00_CHALLENGES/02_DATA_ANALYSIS/PRACTICA_3/PRA3a_captura_tiempo_real_alumni.ipynb#Y146sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df_reddit\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mDataFrame()\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mrusso/OneDrive%20-%20ATSISTEMAS%20SA/Documentos/GitHub/TheBridge_DataScience_PT_ALUMNI_sep22/00_CHALLENGES/02_DATA_ANALYSIS/PRACTICA_3/PRA3a_captura_tiempo_real_alumni.ipynb#Y146sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df_reddit[\u001b[39m'\u001b[39m\u001b[39mtítulo\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39msubreddit_titulos[\u001b[39m1\u001b[39m:]\n",
      "\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mrusso/OneDrive%20-%20ATSISTEMAS%20SA/Documentos/GitHub/TheBridge_DataScience_PT_ALUMNI_sep22/00_CHALLENGES/02_DATA_ANALYSIS/PRACTICA_3/PRA3a_captura_tiempo_real_alumni.ipynb#Y146sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m df_reddit[\u001b[39m'\u001b[39m\u001b[39mvotaciones\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39msubreddit_votaciones[\u001b[39m1\u001b[39m:]\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mrusso/OneDrive%20-%20ATSISTEMAS%20SA/Documentos/GitHub/TheBridge_DataScience_PT_ALUMNI_sep22/00_CHALLENGES/02_DATA_ANALYSIS/PRACTICA_3/PRA3a_captura_tiempo_real_alumni.ipynb#Y146sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m df_reddit[\u001b[39m'\u001b[39m\u001b[39murl\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39msubreddit_url\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\pandas\\core\\frame.py:3978\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n",
      "\u001b[0;32m   3975\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n",
      "\u001b[0;32m   3976\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m   3977\u001b[0m     \u001b[39m# set column\u001b[39;00m\n",
      "\u001b[1;32m-> 3978\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\pandas\\core\\frame.py:4172\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n",
      "\u001b[0;32m   4162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;32m   4163\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m   4164\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n",
      "\u001b[0;32m   4165\u001b[0m \n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m   4170\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n",
      "\u001b[0;32m   4171\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;32m-> 4172\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n",
      "\u001b[0;32m   4174\u001b[0m     \u001b[39mif\u001b[39;00m (\n",
      "\u001b[0;32m   4175\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n",
      "\u001b[0;32m   4176\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;32m   4177\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n",
      "\u001b[0;32m   4178\u001b[0m     ):\n",
      "\u001b[0;32m   4179\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n",
      "\u001b[0;32m   4180\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\pandas\\core\\frame.py:4905\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n",
      "\u001b[0;32m   4902\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n",
      "\u001b[0;32m   4904\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n",
      "\u001b[1;32m-> 4905\u001b[0m     com\u001b[39m.\u001b[39;49mrequire_length_match(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n",
      "\u001b[0;32m   4906\u001b[0m \u001b[39mreturn\u001b[39;00m sanitize_array(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_2d\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mrusso\\Miniconda3\\envs\\the_bridge_22\\lib\\site-packages\\pandas\\core\\common.py:561\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n",
      "\u001b[0;32m    557\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    558\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n",
      "\u001b[0;32m    559\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    560\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n",
      "\u001b[1;32m--> 561\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n",
      "\u001b[0;32m    562\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m    563\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m    564\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m    565\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m    566\u001b[0m     )\n",
      "\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (7) does not match length of index (8)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_reddit=pd.DataFrame()\n",
    "df_reddit['título']=subreddit_titulos[1:]\n",
    "df_reddit['votaciones']=subreddit_votaciones[1:]\n",
    "df_reddit['url']=subreddit_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "DaWKTgJiB02L",
    "outputId": "9cd16d1f-f791-4c79-f022-553cf6f30333"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>título</th>\n",
       "      <th>votaciones</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masks that fit your face and your personality. Shop Now</td>\n",
       "      <td>1.1k</td>\n",
       "      <td>{https://www.reddit.com/r/leagueoflegends/}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is a masterpiece</td>\n",
       "      <td>28.6k</td>\n",
       "      <td>{https://www.reddit.com/r/Damnthatsinteresting/}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Truck drivers, what's a creepy story you've got from the middle of nowhere?</td>\n",
       "      <td>45.9k</td>\n",
       "      <td>{https://www.reddit.com/r/AskReddit/}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE HOUSTON ASTROS HAVE BEEN ELIMINATED FROM 2020 WORLD SERIES CONTENTION</td>\n",
       "      <td>42.9k</td>\n",
       "      <td>{https://www.reddit.com/r/baseball/}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dad of the year</td>\n",
       "      <td>20.5k</td>\n",
       "      <td>{https://www.reddit.com/r/nextfuckinglevel/}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Happy 18th birthday. Buffy can officially legally drink in Australia</td>\n",
       "      <td>37.9k</td>\n",
       "      <td>{https://www.reddit.com/r/aww/}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        título  ...                                               url\n",
       "0  Masks that fit your face and your personality. Shop Now                      ...  {https://www.reddit.com/r/leagueoflegends/}     \n",
       "1  this is a masterpiece                                                        ...  {https://www.reddit.com/r/Damnthatsinteresting/}\n",
       "2  Truck drivers, what's a creepy story you've got from the middle of nowhere?  ...  {https://www.reddit.com/r/AskReddit/}           \n",
       "3  THE HOUSTON ASTROS HAVE BEEN ELIMINATED FROM 2020 WORLD SERIES CONTENTION    ...  {https://www.reddit.com/r/baseball/}            \n",
       "4  Dad of the year                                                              ...  {https://www.reddit.com/r/nextfuckinglevel/}    \n",
       "5  Happy 18th birthday. Buffy can officially legally drink in Australia         ...  {https://www.reddit.com/r/aww/}                 \n",
       "\n",
       "[6 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df_reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tD5CuNtYxtPb"
   },
   "source": [
    "En este ejemplo, hemos podido scrapear el contenido inicial que aparece en la página de Reddit. No obstante, el contenido que encontramos al hacer _scrolldown_ no lo hemos podido capturar.  \n",
    "\n",
    "Para poder capturar el resto de entradas, podemos hacer uso de la funcion *render()* de la librería *Request-html*. No obstante, para el uso de esta función es necesario iniciar una sesión en modo asincrona. Los Jupyter notebooks presentan ciertos problemas para trabajar de esta forma y requieren la instalación de _Chromium_; es por eso que en esta PEC no lo vamos a ver. A pesar de ello, a modo explicativo, se adjunta el código que se requeriría:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "import asyncio\n",
    "from requests_html import AsyncHTMLSession\n",
    "\n",
    "asession = AsyncHTMLSession()\n",
    "\n",
    "async def get_results():\n",
    "    r = await asession.get('https://reddit.com')\n",
    "    r.html.arender(scrolldown=10, sleep=1)\n",
    "    return r\n",
    "\n",
    "respuesta = asession.run(get_results)\n",
    "\n",
    "asession.close()\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p95Elq6CIWDQ"
   },
   "source": [
    "Otro ejemplo es la obtención de información de una página que recoge los memes más relevantes en la actualidad (www.knowyourmeme.com). En esta página, a diferencia del ejemplo anterior, el contenido está organizado por páginas y se puede acceder a ellas a través de los típicos botones de páagina 1, 2, 3,... Vamos a ver, por tanto, la respuesta que nos devolvería requests-html en este caso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KRonTtQKoHk"
   },
   "outputs": [],
   "source": [
    "# Cargar librería\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "# Iniciar sesión\n",
    "session = HTMLSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "UwVnuDSNLOX1",
    "outputId": "a16a1e0d-796f-4d8b-a3dd-48558e4c85aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :  <HTML url='https://knowyourmeme.com/'>\n",
      "2 :  <HTML url='https://knowyourmeme.com/page/2'>\n",
      "3 :  <HTML url='https://knowyourmeme.com/page/3'>\n",
      "4 :  <HTML url='https://knowyourmeme.com/page/4'>\n",
      "5 :  <HTML url='https://knowyourmeme.com/page/5'>\n",
      "6 :  <HTML url='https://knowyourmeme.com/page/6'>\n",
      "7 :  <HTML url='https://knowyourmeme.com/page/7'>\n",
      "8 :  <HTML url='https://knowyourmeme.com/page/8'>\n",
      "9 :  <HTML url='https://knowyourmeme.com/page/9'>\n",
      "10 :  <HTML url='https://knowyourmeme.com/page/10'>\n"
     ]
    }
   ],
   "source": [
    "r2= session.get('https://knowyourmeme.com/')\n",
    "\n",
    "total_pags_scrap=10\n",
    "page=0\n",
    "for html in r2.html:\n",
    "    page+=1\n",
    "    print(page, ': ' ,html)\n",
    "    if page==total_pags_scrap:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxHydcKDMkKn"
   },
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1LJBx70NqQ4"
   },
   "source": [
    "En este caso, la respuesta contine los html asociados a las diferentes páginas en las que está organizado el contenido. Como podéis observar, en el bucle se ha introducido un _break_. Esto es porque knowyourmeme.com ofrece la posibilidad de revisar los memes contenidos en  hasta más de 9500 páginas. Esperar a scrapear este total de páginas nos llevaría mucho más tiempo y no es el objetivo de esta PEC.\n",
    "\n",
    "Considerando los ejemplos que se han facilitado, realizar el ejercicio práctico 3 para explorar la librería Requests-html y familiarizarse con los selectores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9r0z0MdxbSs4"
   },
   "source": [
    "### **Ejercicio práctico 2 (vuelos real time)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTucSQJybd9F"
   },
   "source": [
    "La API OpenSky (https://opensky-network.org/apidoc/rest.html) permite recuperar información del estado de los vuelos o aeronaves actualizados en tiempo real. Además, los usuarios con autorización pueden tener acceso a datos históricos de la última hora, indicando el intervalo de tiempo cómo un parámetro más en la consulta. Para esta actividad vamos a utilizar la versión básica de esta API y por tanto, no vamos a tener opción de seleccionar el intervalo de tiempo de interés. No obstante, para la finalidad de la actividad, no va a hacer falta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwHLlsi7bjiW"
   },
   "source": [
    "En este ejercicio se solicita al estudiante que, después de la revisión de la documentación de la API, obtenga el estado del aire (aviones y aeronaves) durante una secuencia de 10 consultas. Para ello, se recomienda que se implente un bucle donde en cada iteración se solicite información a la API y se esperen 10 segundos entre una iteración y la siguiente. La solicitud a la API se pide que se realice mediante la librería requests, con la que se trabajó en la PEC anterior. \n",
    "\n",
    "Mencionar que esta API permite añadir en la solicitud una acotación del espacio a considerar. La acotación del espacio aéreo se corresponderá con los siguientes valores de latitud i longitud:\n",
    "\n",
    "> lat_max= 72.822950; \n",
    "lon_min= -17.000158;\n",
    "lat_min= 35.550423;\n",
    "lon_max= 44.022436\n",
    "\n",
    "Para cada consulta, se solicita obtener el número total de aeronaves agrupadas por país de origen de la aeronave. Para ello, será necesario revisar la documentación de la API y explorar la forma de la respuesta para determinar cuál es el campo de interés de los datos scrapeados para la realización del ejercicio. \n",
    "Además del número de aeronaves por país de origen, en cada iteración también hay que almacenar el valor temporal de la consulta (es decir, el campo 'time', que viene dado en segundos ).\n",
    "\n",
    "Con el resultado de las 10 iteraciones, se solicita que:\n",
    "- Se cree un dataframe (df_vuelos) que recoja el número total de aeronaves por cada país de origen (filas) para cada instante de tiempo (columnas). \n",
    "- Se añada una columna 'mean_flights' y una 'percen_flights' que representen respectivamente el valor medio de vuelos de cada país durante el intervalo de tiempo considerado con las 10 iteraciones y el porcentaje del país respecto al total.\n",
    "- Representar con un diagrama de barras el número de vuelos medios de los 15 países con mayor media de vuelos en el intervalo de tiempo considerado. El eje X, se corresponderá con el país y el eje Y con el valor de vuelos medios. Mostrar los valores ordenados de forma ascendiente. Mostrar también el porcentaje agregado de estos 15 países respecto al total.\n",
    "- Repetir el putno anterior para los 15 países con menor media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deZfeuJ6n67S"
   },
   "source": [
    "NOTAS PARA LA REALIZACIÓN DE LAS SOLICITUDES Y LA CREACIÓN DEL DATAFRAME: \n",
    "- Para considerar el tiempo de espera, como ya hemos hecho en casos anteriores, se recomienda el uso de la librería time y su función time.sleep(n_segundos).\n",
    "- Debido a que estamos usando la versión gratuita de la API, el tiempo no se actualiza de forma constante y puede que haya solicitudes cuyo valor de 'time' es el mismo. En este caso, no almacenar el valor o eliminar después los valores cuyo 'time' esté duplicado. Esto es para que cuando calculemos la media, no tenga efecto en la misma el valor duplicado.\n",
    "- _Sugerencia:_ En cada iteración se puede crear un dataframe auxiliar con los resultados de la solicitud y cuyas columnas sean 'Country' y 'N' para el 'time' evaluado en dicha iteración. Después, con este dataframe auxiliar, ir hacido join o merge al dataframe glogal que contendrá los resultados de todas las iteraciones. Si se sigue esta sugerencia, utilizar la versión 'outer' de la función merge de fomra que se consideren todos los países que han aparecido en todas las iteraciones aunque en una determinada iteración no hubiese aeronaves de alguno de ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEEfCaAvb69-"
   },
   "outputs": [],
   "source": [
    "#Cargar librerias\n",
    "import requests\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir limites Europa\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hs1Hq70Tb_DU"
   },
   "outputs": [],
   "source": [
    "# Creación de df_vuelos mediante la realización de 10 solicitudes\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostrar df_vuelos\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear nuevas variables: 'mean_flights' y 'percen_flights'\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafico de barras (15 países con más vuelos) + calcular porcentaje de esos 15 primeros paises\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZgLoHcjcBWj"
   },
   "outputs": [],
   "source": [
    "# Grafico de barras (15 países con menos vuelos)\n",
    "#TODO\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ZYIvtVsuFTrD"
   ],
   "name": "CyPD_PEC3_2020_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('the_bridge_22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c79a6bee0cc3880bade9bffe4f2ea0bcfc562fd81915ef930f986f3657b2ce7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
